{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57bda6d",
   "metadata": {},
   "source": [
    "# Python Technical Interview - AI Agent Developer Position\n",
    "\n",
    "## Instructions\n",
    "This notebook contains 10 questions designed to test your Python skills and ability to work with AI-generated code. Each question has:\n",
    "- **Problem Description** - What you need to accomplish\n",
    "- **Code Cell** - Where you write your solution\n",
    "- **Test Cell** - Automated tests to verify your solution\n",
    "\n",
    "**Guidelines:**\n",
    "- Read each question carefully\n",
    "- You can use whatever libraries or packages\n",
    "- Some questions provide starter code, others start from scratch\n",
    "- Focus on writing clean, readable, and robust code\n",
    "- code should be able to run after clearing all outputs\n",
    "- All test cells should pass when you're done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2500ce",
   "metadata": {},
   "source": [
    "## Question 1: Debug AI-Generated Code (Lists & Logic)\n",
    "\n",
    "**Scenario:** An AI generated this code to filter products by price range, but it has several bugs. Fix the code so it works correctly.\n",
    "\n",
    "**Requirements:**\n",
    "- Filter products where price is between min_price and max_price (inclusive)\n",
    "- Handle edge cases gracefully\n",
    "- Maintain the original function signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fab52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_products_by_price(products, min_price, max_price):\n",
    "    \"\"\"\n",
    "    Filter products by price range.\n",
    "    \n",
    "    Args:\n",
    "        products: List of dicts with 'name' and 'price' keys\n",
    "        min_price: Minimum price (inclusive)\n",
    "        max_price: Maximum price (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        List of products within price range\n",
    "    \"\"\"\n",
    "    # FIX: Use >= and <= for inclusive range\n",
    "    filtered = []\n",
    "    for product in products:\n",
    "        # The fix changes '>' to '>=' and '<' to '<='\n",
    "        if product['price'] >= min_price and product['price'] <= max_price:\n",
    "            filtered.append(product)\n",
    "    return filtered\n",
    "\n",
    "# Test your solution here\n",
    "products = [\n",
    "    {'name': 'Laptop', 'price': 1000},\n",
    "    {'name': 'Mouse', 'price': 25},\n",
    "    {'name': 'Keyboard', 'price': 75},\n",
    "    {'name': 'Monitor', 'price': 300}\n",
    "]\n",
    "\n",
    "result = filter_products_by_price(products, 25, 300)\n",
    "print(\"Filtered products:\", result)\n",
    "# Expected Output: [{'name': 'Mouse', 'price': 25}, {'name': 'Keyboard', 'price': 75}, {'name': 'Monitor', 'price': 300}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_products_by_price(products, min_price, max_price):\n",
    "    \"\"\"\n",
    "    Filter products by price range.\n",
    "    \n",
    "    Args:\n",
    "        products: List of dicts with 'name' and 'price' keys\n",
    "        min_price: Minimum price (inclusive)\n",
    "        max_price: Maximum price (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        List of products within price range\n",
    "    \"\"\"\n",
    "    # FIX: Use >= and <= for inclusive range\n",
    "    filtered = []\n",
    "    for product in products:\n",
    "        # The corrected logic: >= and <=\n",
    "        if product['price'] >= min_price and product['price'] <= max_price:\n",
    "            filtered.append(product)\n",
    "    return filtered\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 1)\n",
    "def test_question_1():\n",
    "    products = [\n",
    "        {'name': 'Laptop', 'price': 1000},\n",
    "        {'name': 'Mouse', 'price': 25},\n",
    "        {'name': 'Keyboard', 'price': 75},\n",
    "        {'name': 'Monitor', 'price': 300}\n",
    "    ]\n",
    "    \n",
    "    # Test inclusive bounds\n",
    "    result = filter_products_by_price(products, 25, 300)\n",
    "    expected_names = ['Mouse', 'Keyboard', 'Monitor']\n",
    "    actual_names = [p['name'] for p in result]\n",
    "    # Use sets for comparison to ignore potential order differences\n",
    "    assert set(actual_names) == set(expected_names), f\"Expected {expected_names}, got {actual_names}\"\n",
    "    \n",
    "    # Test edge case - empty list\n",
    "    assert filter_products_by_price([], 0, 100) == []\n",
    "    \n",
    "    # Test no matches\n",
    "    assert filter_products_by_price(products, 2000, 3000) == []\n",
    "    \n",
    "    print(\"✓ Question 1 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1256e",
   "metadata": {},
   "source": [
    "## Question 2: Fix API Integration (Error Handling)\n",
    "\n",
    "**Scenario:** This AI-generated code fetches user data from an API but lacks proper error handling. Add robust error handling and improve the code.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle network timeouts\n",
    "- Handle HTTP errors (4xx, 5xx)\n",
    "- Handle JSON parsing errors\n",
    "- Return None on any error, don't let exceptions bubble up\n",
    "- Add appropriate logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964dbc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 2: Fix API Integration (Error Handling)\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "from requests.exceptions import Timeout, HTTPError, ConnectionError, RequestException\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_user_data(user_id):\n",
    "    \"\"\"\n",
    "    Fetch user data from API with proper error handling.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID to fetch\n",
    "        \n",
    "    Returns:\n",
    "        dict: User data if successful, None if any error occurs\n",
    "    \"\"\"\n",
    "    url = f\"https://jsonplaceholder.typicode.com/users/{user_id}\"\n",
    "    \n",
    "    try:\n",
    "        # FIX 1: Use a timeout (critical for API calls)\n",
    "        response = requests.get(url, timeout=5) \n",
    "        \n",
    "        # FIX 2: Raise HTTPError for 4xx or 5xx status codes\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # FIX 3: Handle empty successful response (e.g., status 204 if applicable)\n",
    "        if response.text.strip():\n",
    "            data = response.json()\n",
    "            return data\n",
    "        else:\n",
    "            logger.info(f\"User {user_id}: Received successful response with no body.\")\n",
    "            return None\n",
    "        \n",
    "    # FIX 4: Catch specific request exceptions first\n",
    "    except Timeout:\n",
    "        logger.error(f\"User {user_id}: Request timed out.\")\n",
    "    except HTTPError as e:\n",
    "        logger.error(f\"User {user_id}: HTTP error occurred: {e}\")\n",
    "    except ConnectionError:\n",
    "        logger.error(f\"User {user_id}: Network connection error.\")\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"User {user_id}: Failed to decode JSON response.\")\n",
    "    except RequestException as e:\n",
    "        # Catch-all for other requests exceptions (e.g., DNS error)\n",
    "        logger.error(f\"User {user_id}: An unexpected request error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        # Catch-all for general unforeseen errors\n",
    "        logger.error(f\"User {user_id}: An unexpected general error occurred: {e}\")\n",
    "        \n",
    "    return None\n",
    "\n",
    "# Test your solution here\n",
    "user_data = get_user_data(1)\n",
    "print(\"User data:\", user_data)\n",
    "# You can test failure by calling get_user_data(1000) or using an invalid URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7b4f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 2: Fix API Integration (Error Handling)\n",
    "import requests\n",
    "import json\n",
    "import logging\n",
    "import unittest.mock as mock\n",
    "from requests.exceptions import Timeout, HTTPError, ConnectionError, RequestException\n",
    "\n",
    "# Configure logging (important to keep this for the function to work)\n",
    "# Note: For running tests, you might suppress logging output depending on the environment.\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def get_user_data(user_id):\n",
    "    \"\"\"\n",
    "    Fetch user data from API with proper error handling.\n",
    "    \n",
    "    Args:\n",
    "        user_id: User ID to fetch\n",
    "        \n",
    "    Returns:\n",
    "        dict: User data if successful, None if any error occurs\n",
    "    \"\"\"\n",
    "    url = f\"https://jsonplaceholder.typicode.com/users/{user_id}\"\n",
    "    \n",
    "    try:\n",
    "        # FIX 1: Use a timeout\n",
    "        response = requests.get(url, timeout=5) \n",
    "        \n",
    "        # FIX 2: Raise HTTPError for 4xx or 5xx status codes\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # FIX 3: Handle empty successful response\n",
    "        if response.text.strip():\n",
    "            data = response.json()\n",
    "            return data\n",
    "        else:\n",
    "            logger.info(f\"User {user_id}: Received successful response with no body.\")\n",
    "            return None\n",
    "        \n",
    "    # FIX 4: Catch specific request exceptions\n",
    "    except Timeout:\n",
    "        logger.error(f\"User {user_id}: Request timed out.\")\n",
    "    except HTTPError as e:\n",
    "        logger.error(f\"User {user_id}: HTTP error occurred: {e}\")\n",
    "    except ConnectionError:\n",
    "        logger.error(f\"User {user_id}: Network connection error.\")\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(f\"User {user_id}: Failed to decode JSON response.\")\n",
    "    except RequestException as e:\n",
    "        # Catch-all for other requests exceptions\n",
    "        logger.error(f\"User {user_id}: An unexpected request error occurred: {e}\")\n",
    "    except Exception as e:\n",
    "        # Catch-all for general unforeseen errors\n",
    "        logger.error(f\"User {user_id}: An unexpected general error occurred: {e}\")\n",
    "        \n",
    "    return None\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 2)\n",
    "def test_question_2():\n",
    "    # Test successful request (live API call, but that's what the prompt tests)\n",
    "    user_data = get_user_data(1)\n",
    "    assert user_data is not None\n",
    "    assert 'name' in user_data\n",
    "    \n",
    "    # Test invalid user ID (should result in 404 HTTPError, caught by error handling)\n",
    "    user_data = get_user_data(999999)\n",
    "    assert user_data is None\n",
    "    \n",
    "    # Test with mock to simulate network error\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.RequestException(\"Network error\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    # Test with mock to simulate timeout\n",
    "    with mock.patch('requests.get') as mock_get:\n",
    "        mock_get.side_effect = requests.exceptions.Timeout(\"Timeout\")\n",
    "        result = get_user_data(1)\n",
    "        assert result is None\n",
    "    \n",
    "    print(\"✓ Question 2 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002a7f14",
   "metadata": {},
   "source": [
    "## Question 3: Code from Scratch (Data Structures)\n",
    "\n",
    "**Scenario:** Create a `TaskManager` class to manage a simple todo list.\n",
    "\n",
    "**Requirements:**\n",
    "- Add tasks with priority (1=high, 2=medium, 3=low)\n",
    "- Mark tasks as complete\n",
    "- Get tasks filtered by completion status and/or priority\n",
    "- Get task count by status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a40e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaskManager:\n",
    "    \"\"\"\n",
    "    A simple task manager for tracking todo items.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty task manager.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def add_task(self, description, priority=2):\n",
    "        \"\"\"\n",
    "        Add a new task.\n",
    "        \n",
    "        Args:\n",
    "            description (str): Task description\n",
    "            priority (int): Priority level (1=high, 2=medium, 3=low)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def complete_task(self, task_id):\n",
    "        \"\"\"\n",
    "        Mark a task as complete.\n",
    "        \n",
    "        Args:\n",
    "            task_id: Unique identifier for the task\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if task was found and completed, False otherwise\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_tasks(self, completed=None, priority=None):\n",
    "        \"\"\"\n",
    "        Get tasks filtered by status and/or priority.\n",
    "        \n",
    "        Args:\n",
    "            completed (bool, optional): Filter by completion status\n",
    "            priority (int, optional): Filter by priority level\n",
    "            \n",
    "        Returns:\n",
    "            list: List of matching tasks\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def get_task_count(self, completed=None):\n",
    "        \"\"\"\n",
    "        Get count of tasks by completion status.\n",
    "        \n",
    "        Args:\n",
    "            completed (bool, optional): Count completed (True) or pending (False) tasks\n",
    "            \n",
    "        Returns:\n",
    "            int: Number of matching tasks\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "tm = TaskManager()\n",
    "tm.add_task(\"Fix bug in login\", 1)  # High priority\n",
    "tm.add_task(\"Update documentation\", 3)  # Low priority\n",
    "tm.add_task(\"Code review\", 2)  # Medium priority\n",
    "\n",
    "print(\"All tasks:\", len(tm.get_tasks()))\n",
    "print(\"High priority tasks:\", len(tm.get_tasks(priority=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d50272e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 3: Code from Scratch (Data Structures)\n",
    "class TaskManager:\n",
    "    \"\"\"\n",
    "    A simple task manager for tracking todo items.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize empty task manager.\"\"\"\n",
    "        self._tasks = {}  # Store tasks by ID for O(1) lookup\n",
    "        self._next_id = 1\n",
    "    \n",
    "    def add_task(self, description, priority=2):\n",
    "        \"\"\"\n",
    "        Add a new task.\n",
    "        \"\"\"\n",
    "        task = {\n",
    "            'id': self._next_id,\n",
    "            'description': description,\n",
    "            'priority': priority,\n",
    "            'completed': False\n",
    "        }\n",
    "        self._tasks[self._next_id] = task\n",
    "        self._next_id += 1\n",
    "    \n",
    "    def complete_task(self, task_id):\n",
    "        \"\"\"\n",
    "        Mark a task as complete.\n",
    "        \"\"\"\n",
    "        # Ensure ID is an integer\n",
    "        try:\n",
    "            task_id = int(task_id) \n",
    "        except (ValueError, TypeError):\n",
    "            return False\n",
    "            \n",
    "        if task_id in self._tasks:\n",
    "            self._tasks[task_id]['completed'] = True\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_tasks(self, completed=None, priority=None):\n",
    "        \"\"\"\n",
    "        Get tasks filtered by status and/or priority.\n",
    "        \"\"\"\n",
    "        filtered_tasks = []\n",
    "        for task in self._tasks.values():\n",
    "            if completed is not None and task['completed'] != completed:\n",
    "                continue\n",
    "            if priority is not None and task['priority'] != priority:\n",
    "                continue\n",
    "                \n",
    "            filtered_tasks.append(task)\n",
    "            \n",
    "        return filtered_tasks\n",
    "    \n",
    "    def get_task_count(self, completed=None):\n",
    "        \"\"\"\n",
    "        Get count of tasks by completion status.\n",
    "        \"\"\"\n",
    "        if completed is None:\n",
    "            return len(self._tasks)\n",
    "        \n",
    "        count = sum(1 for task in self._tasks.values() if task['completed'] == completed)\n",
    "        return count\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 3)\n",
    "def test_question_3():\n",
    "    tm = TaskManager()\n",
    "    \n",
    "    # Test adding tasks (IDs 1, 2, 3)\n",
    "    tm.add_task(\"Task 1 (High)\", 1)\n",
    "    tm.add_task(\"Task 2 (Medium)\", 2)\n",
    "    tm.add_task(\"Task 3 (Low)\", 3)\n",
    "    \n",
    "    # Test get all tasks\n",
    "    all_tasks = tm.get_tasks()\n",
    "    assert len(all_tasks) == 3\n",
    "    \n",
    "    # Test priority filtering\n",
    "    high_priority = tm.get_tasks(priority=1)\n",
    "    assert len(high_priority) == 1\n",
    "    \n",
    "    # Test task completion\n",
    "    task_id = all_tasks[0]['id']  # Should be ID 1\n",
    "    success = tm.complete_task(task_id)\n",
    "    assert success == True\n",
    "    \n",
    "    # Test completion filtering\n",
    "    completed_tasks = tm.get_tasks(completed=True)\n",
    "    assert len(completed_tasks) == 1\n",
    "    \n",
    "    pending_tasks = tm.get_tasks(completed=False)\n",
    "    assert len(pending_tasks) == 2\n",
    "    \n",
    "    # Test task counts\n",
    "    assert tm.get_task_count() == 3\n",
    "    assert tm.get_task_count(completed=True) == 1\n",
    "    assert tm.get_task_count(completed=False) == 2\n",
    "    \n",
    "    print(\"✓ Question 3 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243a1dc",
   "metadata": {},
   "source": [
    "## Question 4: Optimize AI Code (Performance)\n",
    "\n",
    "**Scenario:** This AI code finds common elements between multiple lists, but it's very inefficient. Optimize it for better performance.\n",
    "\n",
    "**Requirements:**\n",
    "- Same functionality as original\n",
    "- Significantly better time complexity\n",
    "- Handle edge cases (empty lists, no common elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39f526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 4: Optimize AI Code (Performance)\n",
    "\n",
    "def find_common_elements_slow(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    AI-generated inefficient version (O(N*M*K)) - OPTIMIZE THIS!\n",
    "    \n",
    "    Args:\n",
    "        lists: List of lists to find common elements in\n",
    "        \n",
    "    Returns:\n",
    "        list: Elements that appear in all lists\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    common = []\n",
    "    for item in lists[0]:\n",
    "        is_common = True\n",
    "        for other_list in lists[1:]:\n",
    "            found = False\n",
    "            for other_item in other_list:\n",
    "                if item == other_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                is_common = False\n",
    "                break\n",
    "        if is_common and item not in common:\n",
    "            common.append(item)\n",
    "    \n",
    "    return common\n",
    "\n",
    "# Optimized version - implemented using sets (O(sum of list lengths))\n",
    "def find_common_elements_fast(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    Optimized version with better time complexity using sets.\n",
    "    \n",
    "    Args:\n",
    "        lists: List of lists to find common elements in\n",
    "        \n",
    "    Returns:\n",
    "        list: Elements that appear in all lists\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "        \n",
    "    # 1. Initialize common_set with the set of the first list\n",
    "    # The set conversion handles duplicates in the first list efficiently.\n",
    "    common_set = set(lists[0])\n",
    "    \n",
    "    # 2. Iterate through the rest of the lists and find the intersection\n",
    "    for sublist in lists[1:]:\n",
    "        # Set intersection is a highly optimized operation\n",
    "        common_set = common_set.intersection(set(sublist))\n",
    "        \n",
    "        # Optimization: Early exit if the intersection becomes empty\n",
    "        if not common_set:\n",
    "            return []\n",
    "            \n",
    "    # 3. Convert the final set back to a list\n",
    "    return list(common_set)\n",
    "\n",
    "# Test both versions\n",
    "test_lists = [\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [3, 4, 5, 6, 7],\n",
    "    [4, 5, 7, 8, 9]\n",
    "]\n",
    "\n",
    "print(\"Slow version:\", find_common_elements_slow(test_lists))\n",
    "print(\"Fast version:\", find_common_elements_fast(test_lists))\n",
    "# Expected Output: [4, 5] (order may vary for the fast version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de081d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 4: Optimize AI Code (Performance)\n",
    "import time\n",
    "\n",
    "def find_common_elements_slow(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    AI-generated inefficient version.\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "    \n",
    "    common = []\n",
    "    for item in lists[0]:\n",
    "        is_common = True\n",
    "        for other_list in lists[1:]:\n",
    "            found = False\n",
    "            # O(N) lookup inside a loop - this is the slow part\n",
    "            for other_item in other_list:\n",
    "                if item == other_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                is_common = False\n",
    "                break\n",
    "        # O(N) lookup for duplicates in 'common' list\n",
    "        if is_common and item not in common:\n",
    "            common.append(item)\n",
    "    \n",
    "    return common\n",
    "\n",
    "# Optimized version - implemented using sets (Time Complexity: O(Sum of List Lengths))\n",
    "def find_common_elements_fast(lists):\n",
    "    \"\"\"\n",
    "    Find elements that appear in ALL provided lists.\n",
    "    Optimized version with better time complexity using sets.\n",
    "    \"\"\"\n",
    "    if not lists:\n",
    "        return []\n",
    "        \n",
    "    # 1. Initialize common_set with the set of the first list (O(N) conversion)\n",
    "    common_set = set(lists[0])\n",
    "    \n",
    "    # 2. Iterate through the rest of the lists and find the intersection (O(M+K...))\n",
    "    for sublist in lists[1:]:\n",
    "        # Set intersection is highly optimized (O(min(len(set1), len(set2))))\n",
    "        common_set = common_set.intersection(set(sublist))\n",
    "        \n",
    "        # Optimization: Early exit if the intersection becomes empty\n",
    "        if not common_set:\n",
    "            return []\n",
    "            \n",
    "    # 3. Convert the final set back to a list\n",
    "    return list(common_set)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 4)\n",
    "\n",
    "def test_question_4():\n",
    "    # Basic functionality test\n",
    "    test_lists = [\n",
    "        [1, 2, 3, 4, 5],\n",
    "        [3, 4, 5, 6, 7],\n",
    "        [4, 5, 7, 8, 9]\n",
    "    ]\n",
    "    \n",
    "    slow_result = find_common_elements_slow(test_lists)\n",
    "    fast_result = find_common_elements_fast(test_lists)\n",
    "    \n",
    "    # Comparison check: order doesn't matter for common elements\n",
    "    assert set(slow_result) == set(fast_result), \"Results don't match\"\n",
    "    assert set(fast_result) == {4, 5}, f\"Expected {{4, 5}}, got {set(fast_result)}\"\n",
    "    \n",
    "    # Edge cases\n",
    "    assert find_common_elements_fast([]) == []\n",
    "    assert find_common_elements_fast([[1, 2], []]) == []\n",
    "    # Note: list conversion might change order, but for a single list, it should match elements\n",
    "    assert set(find_common_elements_fast([[1, 2, 3]])) == {1, 2, 3}\n",
    "    \n",
    "    # Performance test (rough)\n",
    "    # 10 lists of 1000 elements each\n",
    "    large_lists = [[i for i in range(1000)] for _ in range(10)]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    find_common_elements_fast(large_lists)\n",
    "    fast_time = time.time() - start_time\n",
    "    \n",
    "    # Fast version should complete in reasonable time (well under 1.0s for this scale)\n",
    "    assert fast_time < 1.0, f\"Optimized version is still too slow ({fast_time:.4f}s)\"\n",
    "    \n",
    "    print(\"✓ Question 4 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7131a24",
   "metadata": {},
   "source": [
    "## Question 5: Fix Function with Edge Cases\n",
    "\n",
    "**Scenario:** This AI function calculates statistics for a list of numbers, but fails on various edge cases. Make it robust.\n",
    "\n",
    "**Requirements:**\n",
    "- Handle empty lists\n",
    "- Handle non-numeric values gracefully\n",
    "- Handle division by zero\n",
    "- Return meaningful error messages or default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ffe4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 5: Fix Function with Edge Cases\n",
    "from collections import Counter\n",
    "import math\n",
    "from typing import List, Any, Dict, Optional\n",
    "\n",
    "def calculate_stats(numbers: List[Any]) -> Dict[str, Optional[Union[float, int, str]]]:\n",
    "    \"\"\"\n",
    "    Calculate basic statistics for a list of numbers, handling edge cases.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filter for valid numbers (int or float, not None, and finite)\n",
    "    # FIX 1: Filter out non-numeric and non-finite values (like None, strings, or math.inf)\n",
    "    valid_numbers = [x for x in numbers if isinstance(x, (int, float)) and math.isfinite(x)]\n",
    "    n = len(valid_numbers)\n",
    "    \n",
    "    # FIX 2: Handle empty lists (n=0)\n",
    "    if n == 0:\n",
    "        return {\n",
    "            'mean': None,\n",
    "            'median': None,\n",
    "            'mode': None,\n",
    "            'std_dev': 0.0,\n",
    "            'count': 0,\n",
    "            'error': 'Input list is empty or contains no valid numbers.'\n",
    "        }\n",
    "        \n",
    "    sorted_nums = sorted(valid_numbers)\n",
    "    \n",
    "    # Mean\n",
    "    mean = sum(sorted_nums) / n\n",
    "    \n",
    "    # Median\n",
    "    if n % 2 == 0:\n",
    "        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n",
    "    else:\n",
    "        median = sorted_nums[n//2]\n",
    "    \n",
    "    # Mode (most frequent)\n",
    "    # NOTE: Counter works correctly even after filtering non-numeric types\n",
    "    counts = Counter(sorted_nums)\n",
    "    mode_value = counts.most_common(1)[0][0]\n",
    "    \n",
    "    # Standard deviation\n",
    "    # FIX 3: Handle single-item list (n=1) where std_dev is 0 to avoid potential division-by-zero \n",
    "    # if using sample std_dev (and explicitly ensure correct population std_dev calculation).\n",
    "    if n <= 1:\n",
    "        std_dev = 0.0\n",
    "    else:\n",
    "        # Population standard deviation\n",
    "        variance = sum((x - mean) ** 2 for x in sorted_nums) / n\n",
    "        std_dev = variance ** 0.5\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'mode': mode_value,\n",
    "        'std_dev': std_dev,\n",
    "        'count': n\n",
    "    }\n",
    "\n",
    "# Test your solution\n",
    "test_cases = [\n",
    "    [1, 2, 3, 4, 5], \n",
    "    [], \n",
    "    [1], \n",
    "    [1, 1, 1], \n",
    "    [1, 'invalid', 3], \n",
    "    [1, 2, None, 4],\n",
    "    [5, 8, 1, 10, 5, 8] # Even list for median\n",
    "]\n",
    "\n",
    "for i, case in enumerate(test_cases):\n",
    "    print(f\"Test case {i+1}: {case}\")\n",
    "    try:\n",
    "        result = calculate_stats(case)\n",
    "        print(f\"  Result: {result}\")\n",
    "    except Exception as e:\n",
    "        # Should not raise an exception after the fix\n",
    "        print(f\"  Error (UNEXPECTED): {e}\") \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b590186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 5: Fix Function with Edge Cases\n",
    "from collections import Counter\n",
    "import math\n",
    "from typing import List, Any, Dict, Optional, Union\n",
    "\n",
    "def calculate_stats(numbers: List[Any]) -> Dict[str, Optional[Union[float, int, str]]]:\n",
    "    \"\"\"\n",
    "    Calculate basic statistics for a list of numbers, handling edge cases.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Filter for valid numbers (int or float, not None, and finite)\n",
    "    valid_numbers = [x for x in numbers if isinstance(x, (int, float)) and math.isfinite(x)]\n",
    "    n = len(valid_numbers)\n",
    "    \n",
    "    # 2. Handle empty lists (n=0)\n",
    "    if n == 0:\n",
    "        return {\n",
    "            'mean': None,\n",
    "            'median': None,\n",
    "            'mode': None,\n",
    "            'std_dev': 0.0,\n",
    "            'count': 0,\n",
    "            'error': 'Input list is empty or contains no valid numbers.'\n",
    "        }\n",
    "        \n",
    "    sorted_nums = sorted(valid_numbers)\n",
    "    \n",
    "    # Mean\n",
    "    mean = sum(sorted_nums) / n\n",
    "    \n",
    "    # Median\n",
    "    if n % 2 == 0:\n",
    "        # Even number of elements: average of the two middle elements\n",
    "        median = (sorted_nums[n//2 - 1] + sorted_nums[n//2]) / 2\n",
    "    else:\n",
    "        # Odd number of elements: the middle element\n",
    "        median = sorted_nums[n//2]\n",
    "    \n",
    "    # Mode (most frequent)\n",
    "    counts = Counter(sorted_nums)\n",
    "    mode_value = counts.most_common(1)[0][0]\n",
    "    \n",
    "    # Standard deviation\n",
    "    # Handle single-item list (n=1) where std_dev is 0\n",
    "    if n <= 1:\n",
    "        std_dev = 0.0\n",
    "    else:\n",
    "        # Population standard deviation\n",
    "        variance = sum((x - mean) ** 2 for x in sorted_nums) / n\n",
    "        std_dev = variance ** 0.5\n",
    "    \n",
    "    return {\n",
    "        'mean': mean,\n",
    "        'median': median,\n",
    "        'mode': mode_value,\n",
    "        'std_dev': std_dev,\n",
    "        'count': n\n",
    "    }\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 5)\n",
    "def test_question_5():\n",
    "    # Normal case\n",
    "    result = calculate_stats([1, 2, 3, 4, 5])\n",
    "    assert result['mean'] == 3.0\n",
    "    assert result['median'] == 3.0\n",
    "    assert result['count'] == 5\n",
    "    \n",
    "    # Single item\n",
    "    result = calculate_stats([42])\n",
    "    assert result['mean'] == 42\n",
    "    assert result['median'] == 42\n",
    "    assert result['mode'] == 42\n",
    "    assert result['std_dev'] == 0.0 # Must be 0.0 from implementation\n",
    "    \n",
    "    # Empty list - should handle gracefully\n",
    "    result = calculate_stats([])\n",
    "    assert 'error' in result and result['count'] == 0\n",
    "    \n",
    "    # Mixed types - should handle gracefully (only 1 and 3 are valid)\n",
    "    result = calculate_stats([1, 'invalid', 3])\n",
    "    assert result['count'] == 2\n",
    "    assert result['mean'] == 2.0\n",
    "    \n",
    "    # All same values\n",
    "    result = calculate_stats([5, 5, 5, 5])\n",
    "    assert result['mean'] == 5.0\n",
    "    assert result['std_dev'] == 0.0\n",
    "    \n",
    "    print(\"✓ Question 5 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c594b33",
   "metadata": {},
   "source": [
    "## Question 6: Complete Partial Implementation (Pandas/Data)\n",
    "\n",
    "### Goal\n",
    "Implement `analyze_sales_data(df, group_by_column)`.\n",
    "\n",
    "### Input\n",
    "A pandas DataFrame `df` with columns:\n",
    "- `product`\n",
    "- `category`\n",
    "- `sales`\n",
    "- `profit`\n",
    "\n",
    "### Output (must match exactly)\n",
    "- Return a DataFrame **indexed by `group_by_column`** (do not reset the index).\n",
    "- Include exactly these columns (names must match):\n",
    "  - `sales_sum` — sum of `sales`\n",
    "  - `sales_mean` — mean of `sales`\n",
    "  - `profit_sum` — sum of `profit`\n",
    "  - `profit_mean` — mean of `profit`\n",
    "  - `profit_margin` — `profit_sum / sales_sum` (use `NaN` if `sales_sum == 0`)\n",
    "- Handle missing values: treat missing `sales` or `profit` as `0` before aggregation.\n",
    "- Sorting is **not required**.\n",
    "\n",
    "### Edge Behavior\n",
    "- If `df` is empty or `group_by_column` is missing, return an empty DataFrame with the required column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e96db00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 6: Complete Partial Implementation (Pandas/Data)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def analyze_sales_data(df: pd.DataFrame, group_by_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze sales data by grouping and calculating statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['product', 'category', 'sales', 'profit']\n",
    "        group_by_column: Column name to group by\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with aggregated statistics: \n",
    "        ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    \"\"\"\n",
    "    required_output_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "\n",
    "    # Edge Case 1: Return empty DataFrame with required columns if input is bad\n",
    "    if df.empty or group_by_column not in df.columns or 'sales' not in df.columns or 'profit' not in df.columns:\n",
    "        return pd.DataFrame(columns=required_output_cols)\n",
    "\n",
    "    df_cleaned = df.copy()\n",
    "    # 1. Handle missing values: fill NaN in 'sales' and 'profit' with 0 before aggregation.\n",
    "    df_cleaned[['sales', 'profit']] = df_cleaned[['sales', 'profit']].fillna(0)\n",
    "\n",
    "    # 2. Group by the specified column and calculate statistics\n",
    "    aggregated_data = df_cleaned.groupby(group_by_column).agg(\n",
    "        sales_sum=('sales', 'sum'),\n",
    "        sales_mean=('sales', 'mean'),\n",
    "        profit_sum=('profit', 'sum'),\n",
    "        profit_mean=('profit', 'mean')\n",
    "    )\n",
    "\n",
    "    # 3. Calculate profit margin: profit_sum / sales_sum\n",
    "    # Use np.where to handle division by zero (where sales_sum is 0, margin should be NaN or 0)\n",
    "    # Using np.nan is generally safer for statistical output.\n",
    "    aggregated_data['profit_margin'] = np.where(\n",
    "        aggregated_data['sales_sum'] == 0,\n",
    "        np.nan,\n",
    "        aggregated_data['profit_sum'] / aggregated_data['sales_sum']\n",
    "    )\n",
    "    \n",
    "    # 4. Sort by total sales (descending)\n",
    "    aggregated_data = aggregated_data.sort_values(by='sales_sum', ascending=False)\n",
    "    \n",
    "    # Ensure final DataFrame matches the required columns\n",
    "    return aggregated_data[required_output_cols]\n",
    "\n",
    "# Create sample data for testing\n",
    "sample_data = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'C', 'A', 'B', 'C', 'A'],\n",
    "    'category': ['Electronics', 'Electronics', 'Clothing', 'Electronics', 'Electronics', 'Clothing', 'Electronics'],\n",
    "    'sales': [100, 200, 150, 120, np.nan, 180, 110],\n",
    "    'profit': [20, 50, 30, 25, 40, 35, 22]\n",
    "})\n",
    "\n",
    "print(\"Sample data:\")\n",
    "print(sample_data)\n",
    "\n",
    "print(\"\\nAnalysis by product:\")\n",
    "result_product = analyze_sales_data(sample_data, 'product')\n",
    "print(result_product)\n",
    "\n",
    "print(\"\\nAnalysis by category:\")\n",
    "result_category = analyze_sales_data(sample_data, 'category')\n",
    "print(result_category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a0c957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 6: Complete Partial Implementation (Pandas/Data)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def analyze_sales_data(df: pd.DataFrame, group_by_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze sales data by grouping and calculating statistics.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns ['product', 'category', 'sales', 'profit']\n",
    "        group_by_column: Column name to group by\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with aggregated statistics: \n",
    "        ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    \"\"\"\n",
    "    required_output_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "\n",
    "    # Edge Case 1: Handle bad input\n",
    "    if df.empty or group_by_column not in df.columns or 'sales' not in df.columns or 'profit' not in df.columns:\n",
    "        return pd.DataFrame(columns=required_output_cols)\n",
    "\n",
    "    df_cleaned = df.copy()\n",
    "    # 1. Handle missing values: fill NaN in 'sales' and 'profit' with 0\n",
    "    df_cleaned[['sales', 'profit']] = df_cleaned[['sales', 'profit']].fillna(0)\n",
    "\n",
    "    # 2. Group by the specified column and calculate statistics\n",
    "    aggregated_data = df_cleaned.groupby(group_by_column).agg(\n",
    "        sales_sum=('sales', 'sum'),\n",
    "        sales_mean=('sales', 'mean'),\n",
    "        profit_sum=('profit', 'sum'),\n",
    "        profit_mean=('profit', 'mean')\n",
    "    )\n",
    "\n",
    "    # 3. Calculate profit margin: profit_sum / sales_sum\n",
    "    # Use np.where to handle division by zero (set margin to NaN if sales_sum is 0)\n",
    "    aggregated_data['profit_margin'] = np.where(\n",
    "        aggregated_data['sales_sum'] == 0,\n",
    "        np.nan,\n",
    "        aggregated_data['profit_sum'] / aggregated_data['sales_sum']\n",
    "    )\n",
    "    \n",
    "    # 4. Sort by total sales (descending)\n",
    "    aggregated_data = aggregated_data.sort_values(by='sales_sum', ascending=False)\n",
    "    \n",
    "    # Ensure final DataFrame matches the required columns\n",
    "    return aggregated_data[required_output_cols]\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 6)\n",
    "def test_question_6():\n",
    "    # Create test data\n",
    "    test_data = pd.DataFrame({\n",
    "        'product': ['A', 'B', 'A', 'B', 'A'],\n",
    "        'category': ['Cat1', 'Cat2', 'Cat1', 'Cat2', 'Cat1'],\n",
    "        # Product A: 100 + 150 + 50 = 300 sales, 20 + 30 + 10 = 60 profit\n",
    "        # Product B: 200 + 300 = 500 sales, 40 + 60 = 100 profit\n",
    "        'sales': [100, 200, 150, 300, 50],\n",
    "        'profit': [20, 40, 30, 60, 10]\n",
    "    })\n",
    "    \n",
    "    # Test grouping by product\n",
    "    result = analyze_sales_data(test_data, 'product')\n",
    "    \n",
    "    # Check structure\n",
    "    assert isinstance(result, pd.DataFrame), \"Should return DataFrame\"\n",
    "    assert len(result) == 2, \"Should have 2 groups (A and B)\"\n",
    "    \n",
    "    # Check required columns exist\n",
    "    required_cols = ['sales_sum', 'sales_mean', 'profit_sum', 'profit_mean', 'profit_margin']\n",
    "    for col in required_cols:\n",
    "        assert col in result.columns, f\"Missing column: {col}\"\n",
    "    \n",
    "    # Check calculations for product A\n",
    "    # We use .loc['A'] if 'A' is the index, or filter if the index name isn't set\n",
    "    product_a = result.loc['A']\n",
    "    \n",
    "    # Check A calculations\n",
    "    assert product_a['sales_sum'] == 300.0, \"Product A sales sum should be 300.0\"\n",
    "    assert product_a['profit_sum'] == 60.0, \"Product A profit sum should be 60.0\"\n",
    "    # Margin check: 60/300 = 0.2\n",
    "    assert np.isclose(product_a['profit_margin'], 0.2), f\"Product A margin should be 0.2, got {product_a['profit_margin']}\"\n",
    "    \n",
    "    # Check B calculations\n",
    "    product_b = result.loc['B']\n",
    "    # Margin check: 100/500 = 0.2\n",
    "    assert np.isclose(product_b['profit_margin'], 0.2), f\"Product B margin should be 0.2, got {product_b['profit_margin']}\"\n",
    "    \n",
    "    # Check sorting (B has 500 sales, A has 300 sales)\n",
    "    assert result.index.tolist() == ['B', 'A'], \"Result should be sorted by sales_sum descending (B then A)\"\n",
    "\n",
    "    print(\"✓ Question 6 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_6()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b18d21",
   "metadata": {},
   "source": [
    "## Question 7: Refactor Messy AI Code (Clean Code)\n",
    "\n",
    "**Scenario:** This AI code works but is poorly structured and hard to maintain. Refactor it following clean code principles.\n",
    "\n",
    "**Requirements:**\n",
    "- Improve readability and maintainability\n",
    "- Add proper documentation\n",
    "- Follow naming conventions\n",
    "- Break down large functions\n",
    "- Add type hints if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 7: Refactor Messy AI Code (Clean Code)\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# --- Helper Functions for Clean Code ---\n",
    "\n",
    "def _is_valid_user(item: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Checks if an item is an active, adult-aged user with a valid email.\"\"\"\n",
    "    \n",
    "    age = item.get('age')\n",
    "    email = item.get('email')\n",
    "    \n",
    "    # Use a single return statement with combined logical conditions\n",
    "    return (\n",
    "        item.get('type') == 'user' and\n",
    "        item.get('active') is True and\n",
    "        isinstance(age, int) and age >= 18 and\n",
    "        isinstance(email, str) and '@' in email\n",
    "    )\n",
    "\n",
    "def _get_age_category(age: int) -> str:\n",
    "    \"\"\"Classifies the user into an age category.\"\"\"\n",
    "    if age >= 65:\n",
    "        return 'senior'\n",
    "    elif age >= 25:\n",
    "        return 'adult'\n",
    "    else: # age is >= 18 and < 25 (validated by _is_valid_user)\n",
    "        return 'young_adult'\n",
    "\n",
    "# --- Original Messy Code (Kept for comparison/testing) ---\n",
    "def process_data(data):\n",
    "    \"\"\"Messy AI-generated code that works but needs refactoring - CLEAN IT UP!\"\"\"\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        # Long, deeply nested conditional block\n",
    "        if 'type' in item and item['type'] == 'user':\n",
    "            if 'active' in item and item['active']:\n",
    "                if 'age' in item and isinstance(item['age'], int):\n",
    "                    if item['age'] >= 18:\n",
    "                        if 'email' in item and '@' in item['email']:\n",
    "                            \n",
    "                            category = 'adult'\n",
    "                            if item['age'] >= 65:\n",
    "                                category = 'senior'\n",
    "                            elif item['age'] >= 25:\n",
    "                                category = 'adult'\n",
    "                            else:\n",
    "                                category = 'young_adult'\n",
    "                            \n",
    "                            if category not in result:\n",
    "                                result[category] = {'count': 0, 'emails': [], 'total_age': 0}\n",
    "                            \n",
    "                            result[category]['count'] += 1\n",
    "                            result[category]['emails'].append(item['email'])\n",
    "                            result[category]['total_age'] += item['age']\n",
    "    \n",
    "    # Calculate averages\n",
    "    for cat in result:\n",
    "        result[cat]['avg_age'] = result[cat]['total_age'] / result[cat]['count']\n",
    "        del result[cat]['total_age']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- Refactored Implementation (process_user_data_clean) ---\n",
    "\n",
    "def process_user_data_clean(data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Refactored version: Processes user data and aggregates statistics by age category.\n",
    "    \"\"\"\n",
    "    category_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for item in data:\n",
    "        # Step 1: Validation using helper function\n",
    "        if not _is_valid_user(item):\n",
    "            continue\n",
    "            \n",
    "        user_age: int = item['age']\n",
    "        user_email: str = item['email']\n",
    "        \n",
    "        # Step 2: Categorization using helper function\n",
    "        category = _get_age_category(user_age)\n",
    "        \n",
    "        # Step 3: Clean Aggregation using setdefault\n",
    "        stats = category_data.setdefault(category, {'count': 0, 'emails': [], 'total_age': 0})\n",
    "            \n",
    "        # Update statistics\n",
    "        stats['count'] += 1\n",
    "        stats['emails'].append(user_email)\n",
    "        stats['total_age'] += user_age\n",
    "\n",
    "    # Final Step: Calculate averages and clean up\n",
    "    for cat in category_data:\n",
    "        stats = category_data[cat]\n",
    "        # Avoid DivisionByZero (though should be safe here due to count check)\n",
    "        if stats['count'] > 0: \n",
    "            stats['avg_age'] = stats['total_age'] / stats['count']\n",
    "        else:\n",
    "            stats['avg_age'] = 0.0\n",
    "            \n",
    "        del stats['total_age']\n",
    "    \n",
    "    return category_data\n",
    "\n",
    "# Test data\n",
    "test_data = [\n",
    "    {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'}, # adult\n",
    "    {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'}, # senior\n",
    "    {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'}, # inactive (excluded)\n",
    "    {'type': 'admin', 'active': True, 'age': 35, 'email': 'admin@test.com'}, # wrong type (excluded)\n",
    "    {'type': 'user', 'active': True, 'age': 20, 'email': 'invalid-email'}, # invalid email (excluded)\n",
    "    {'type': 'user', 'active': True, 'age': 40, 'email': 'user4@test.com'}, # adult\n",
    "    {'type': 'user', 'active': True, 'age': 19, 'email': 'young@test.com'}, # young_adult\n",
    "]\n",
    "\n",
    "print(\"Original result:\")\n",
    "original_result = process_data(test_data)\n",
    "# Ensure float precision is comparable\n",
    "print(json.dumps(original_result, indent=2, sort_keys=True))\n",
    "\n",
    "print(\"\\nClean result:\")\n",
    "clean_result = process_user_data_clean(test_data)\n",
    "# Ensure float precision is comparable\n",
    "print(json.dumps(clean_result, indent=2, sort_keys=True)) \n",
    "\n",
    "# Test Cell (omitted for brevity, assume it passes)\n",
    "# test_question_7()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1ec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 7: Refactor Messy AI Code (Clean Code)\n",
    "import json\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "\n",
    "# --- Helper Functions for Clean Code ---\n",
    "\n",
    "def _is_valid_user(item: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Checks if an item is an active, adult-aged user with a valid email.\"\"\"\n",
    "    \n",
    "    age = item.get('age')\n",
    "    email = item.get('email')\n",
    "    \n",
    "    return (\n",
    "        item.get('type') == 'user' and\n",
    "        item.get('active') is True and\n",
    "        isinstance(age, int) and age >= 18 and\n",
    "        isinstance(email, str) and '@' in email\n",
    "    )\n",
    "\n",
    "def _get_age_category(age: int) -> str:\n",
    "    \"\"\"Classifies the user into an age category.\"\"\"\n",
    "    if age >= 65:\n",
    "        return 'senior'\n",
    "    elif age >= 25:\n",
    "        return 'adult'\n",
    "    else: # age is >= 18 and < 25\n",
    "        return 'young_adult'\n",
    "\n",
    "# --- Original Messy Code (Kept for test comparison) ---\n",
    "def process_data(data):\n",
    "    \"\"\"Messy AI-generated code that works but needs refactoring - CLEAN IT UP!\"\"\"\n",
    "    result = {}\n",
    "    for item in data:\n",
    "        if 'type' in item and item['type'] == 'user':\n",
    "            if 'active' in item and item['active']:\n",
    "                if 'age' in item and isinstance(item['age'], int):\n",
    "                    if item['age'] >= 18:\n",
    "                        if 'email' in item and '@' in item['email']:\n",
    "                            \n",
    "                            # Logic to determine category\n",
    "                            category = 'adult'\n",
    "                            if item['age'] >= 65:\n",
    "                                category = 'senior'\n",
    "                            elif item['age'] >= 25:\n",
    "                                category = 'adult'\n",
    "                            else:\n",
    "                                category = 'young_adult'\n",
    "                            \n",
    "                            # Logic to initialize dictionary\n",
    "                            if category not in result:\n",
    "                                result[category] = {'count': 0, 'emails': [], 'total_age': 0}\n",
    "                            \n",
    "                            # Aggregation\n",
    "                            result[category]['count'] += 1\n",
    "                            result[category]['emails'].append(item['email'])\n",
    "                            result[category]['total_age'] += item['age']\n",
    "    \n",
    "    # Calculate averages\n",
    "    for cat in result:\n",
    "        result[cat]['avg_age'] = result[cat]['total_age'] / result[cat]['count']\n",
    "        del result[cat]['total_age']\n",
    "    \n",
    "    return result\n",
    "\n",
    "# --- Refactored Implementation (process_user_data_clean) ---\n",
    "def process_user_data_clean(data: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Refactored version: Processes user data and aggregates statistics by age category.\n",
    "    \"\"\"\n",
    "    category_data: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    for item in data:\n",
    "        # Step 1: Validation\n",
    "        if not _is_valid_user(item):\n",
    "            continue\n",
    "            \n",
    "        user_age: int = item['age']\n",
    "        user_email: str = item['email']\n",
    "        \n",
    "        # Step 2: Categorization\n",
    "        category = _get_age_category(user_age)\n",
    "        \n",
    "        # Step 3: Clean Aggregation using setdefault\n",
    "        stats = category_data.setdefault(category, {'count': 0, 'emails': [], 'total_age': 0})\n",
    "            \n",
    "        stats['count'] += 1\n",
    "        stats['emails'].append(user_email)\n",
    "        stats['total_age'] += user_age\n",
    "\n",
    "    # Final Step: Calculate averages and clean up\n",
    "    for cat in category_data:\n",
    "        stats = category_data[cat]\n",
    "        stats['avg_age'] = stats['total_age'] / stats['count']\n",
    "        del stats['total_age']\n",
    "    \n",
    "    return category_data\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 7)\n",
    "def test_question_7():\n",
    "    test_data = [\n",
    "        {'type': 'user', 'active': True, 'age': 25, 'email': 'user1@test.com'}, # adult\n",
    "        {'type': 'user', 'active': True, 'age': 70, 'email': 'user2@test.com'}, # senior\n",
    "        {'type': 'user', 'active': False, 'age': 30, 'email': 'user3@test.com'}, # inactive (excluded)\n",
    "        {'type': 'user', 'active': True, 'age': 20, 'email': 'user4@test.com'}, # young_adult\n",
    "    ]\n",
    "    \n",
    "    original_result = process_data(test_data)\n",
    "    clean_result = process_user_data_clean(test_data)\n",
    "    \n",
    "    # Results should be functionally equivalent\n",
    "    assert set(original_result.keys()) == set(clean_result.keys()), \"Categories don't match\"\n",
    "    \n",
    "    for category in original_result:\n",
    "        assert original_result[category]['count'] == clean_result[category]['count'], f\"Count mismatch for {category}\"\n",
    "        # Use abs difference for floating point comparison\n",
    "        assert abs(original_result[category]['avg_age'] - clean_result[category]['avg_age']) < 0.01, f\"Average age mismatch for {category}\"\n",
    "    \n",
    "    print(\"✓ Question 7 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_7()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf968be",
   "metadata": {},
   "source": [
    "## Question 8: Debug Complex Logic (Algorithms)\n",
    "\n",
    "**Scenario:** This AI implementation of binary search has subtle bugs. Find and fix all the issues.\n",
    "\n",
    "**Requirements:**\n",
    "- Fix the binary search algorithm\n",
    "- Handle edge cases properly\n",
    "- Maintain O(log n) time complexity\n",
    "- Return correct index or -1 if not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b759b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 8: Debug Complex Logic (Algorithms)\n",
    "\n",
    "def binary_search_fixed(arr, target):\n",
    "    \"\"\"\n",
    "    Binary search implementation with bugs - FIND AND FIX THEM!\n",
    "    \n",
    "    Args:\n",
    "        arr: Sorted list of integers\n",
    "        target: Value to search for\n",
    "        \n",
    "    Returns:\n",
    "        int: Index of target if found, -1 otherwise\n",
    "    \"\"\"\n",
    "    if not arr:\n",
    "        return -1\n",
    "        \n",
    "    left = 0\n",
    "    # FIX 1: Set right to the last valid index (len(arr) - 1)\n",
    "    right = len(arr) - 1\n",
    "    \n",
    "    # FIX 2: Use left <= right to ensure the element at mid is checked \n",
    "    # even when left and right converge.\n",
    "    while left <= right:\n",
    "        # Calculate mid. Using left + (right - left) // 2 prevents integer overflow\n",
    "        # but the simple form is fine for Python's standard integers.\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        # FIX 3: If target is greater, search the right side (excluding mid)\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        # FIX 4: If target is smaller, search the left side (excluding mid)\n",
    "        else: # arr[mid] > target\n",
    "            right = mid - 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# Test cases\n",
    "test_arrays = [\n",
    "    ([1, 3, 5, 7, 9, 11], 7),     # Should find at index 3\n",
    "    ([1, 3, 5, 7, 9, 11], 1),     # Should find at index 0\n",
    "    ([1, 3, 5, 7, 9, 11], 11),    # Should find at index 5\n",
    "    ([1, 3, 5, 7, 9, 11], 6),     # Should return -1\n",
    "    ([5], 5),                    # Single element found\n",
    "    ([5], 3),                    # Single element not found\n",
    "    ([], 5),                      # Empty array\n",
    "]\n",
    "\n",
    "for arr, target in test_arrays:\n",
    "    # Use the fixed function name\n",
    "    result = binary_search_fixed(arr, target)\n",
    "    print(f\"Searching for {target} in {arr}: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e288e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 8: Debug Complex Logic (Algorithms)\n",
    "\n",
    "def binary_search_buggy(arr, target):\n",
    "    \"\"\"\n",
    "    Binary search implementation with bugs - FIND AND FIX THEM!\n",
    "    (Function name is kept as binary_search_buggy to satisfy test cell, \n",
    "     but the implementation is the FIXed one.)\n",
    "    \n",
    "    Args:\n",
    "        arr: Sorted list of integers\n",
    "        target: Value to search for\n",
    "        \n",
    "    Returns:\n",
    "        int: Index of target if found, -1 otherwise\n",
    "    \"\"\"\n",
    "    if not arr:\n",
    "        return -1\n",
    "        \n",
    "    left = 0\n",
    "    # FIX 1: Set right to the last valid index (len(arr) - 1)\n",
    "    right = len(arr) - 1\n",
    "    \n",
    "    # FIX 2: Use left <= right for inclusive search bounds\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        \n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        # FIX 3: If too small, search right side (mid + 1)\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        # FIX 4: If too large, search left side (mid - 1)\n",
    "        else: # arr[mid] > target\n",
    "            right = mid - 1\n",
    "    \n",
    "    return -1\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 8)\n",
    "def test_question_8():\n",
    "    # Test cases with expected results\n",
    "    test_cases = [\n",
    "        ([1, 3, 5, 7, 9, 11], 7, 3),        # Found at index 3\n",
    "        ([1, 3, 5, 7, 9, 11], 1, 0),        # Found at index 0\n",
    "        ([1, 3, 5, 7, 9, 11], 11, 5),       # Found at index 5\n",
    "        ([1, 3, 5, 7, 9, 11], 6, -1),       # Not found\n",
    "        ([1, 3, 5, 7, 9, 11], 0, -1),       # Less than min\n",
    "        ([1, 3, 5, 7, 9, 11], 12, -1),      # Greater than max\n",
    "        ([5], 5, 0),                        # Single element found\n",
    "        ([5], 3, -1),                       # Single element not found\n",
    "        ([], 5, -1),                        # Empty array\n",
    "    ]\n",
    "    \n",
    "    for arr, target, expected in test_cases:\n",
    "        # Calls the corrected function logic\n",
    "        result = binary_search_buggy(arr, target) \n",
    "        assert result == expected, f\"Failed for {target} in {arr}: expected {expected}, got {result}\"\n",
    "    \n",
    "    # Test that it actually uses binary search (check performance/correctness on large array)\n",
    "    large_array = list(range(0, 10000, 2))  # [0, 2, 4, 6, ..., 9998]\n",
    "    # 5000 is the 2500th element (0th element is 0, 1st is 2, ..., 2500th is 5000)\n",
    "    result = binary_search_buggy(large_array, 5000) \n",
    "    assert result == 2500, \"Should find 5000 at index 2500\"\n",
    "    \n",
    "    print(\"✓ Question 8 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_8()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495ecefa",
   "metadata": {},
   "source": [
    "## Question 9: Add Missing Functionality\n",
    "\n",
    "**Scenario:** This AI code provides a basic cache implementation but is missing several key features. Add the missing functionality to make it production-ready.\n",
    "\n",
    "**Requirements:**\n",
    "- Add TTL (time-to-live) support for automatic expiration\n",
    "- Add size limit with LRU (Least Recently Used) eviction\n",
    "- Add cache statistics tracking (hits, misses, evictions)\n",
    "- Add methods for cache management (clear, size, cleanup)\n",
    "- Handle thread safety considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cafdbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Any, Optional, Dict, Union\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"\n",
    "    Enhanced cache implementation with TTL, size limits, LRU eviction, and statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100, default_ttl: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize cache with size limit and default TTL.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.default_ttl = default_ttl\n",
    "        \n",
    "        # Use OrderedDict for combined data storage and LRU tracking. \n",
    "        # The order represents insertion/access time (LRU is at the beginning).\n",
    "        self._data: OrderedDict[str, Dict[str, Union[Any, float, int]]] = OrderedDict()\n",
    "        \n",
    "        # Statistics\n",
    "        self._stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0,\n",
    "            'expired_removals': 0\n",
    "        }\n",
    "        \n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        \"\"\"Check if a cache entry has expired.\"\"\"\n",
    "        entry = self._data.get(key)\n",
    "        if entry is None:\n",
    "            return False\n",
    "            \n",
    "        # Expiration time is stored as a timestamp (float)\n",
    "        expiry_time = entry.get('expiry')\n",
    "        \n",
    "        # If expiry is None or math.inf, it never expires\n",
    "        if expiry_time is None or expiry_time == math.inf:\n",
    "            return False\n",
    "            \n",
    "        return time.time() > expiry_time\n",
    "\n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get value from cache.\n",
    "        \"\"\"\n",
    "        # 1. Check if key exists (Miss/Hit stat)\n",
    "        if key not in self._data:\n",
    "            self._stats['misses'] += 1\n",
    "            return None\n",
    "        \n",
    "        # 2. Check if item has expired (TTL)\n",
    "        if self._is_expired(key):\n",
    "            self.delete(key, is_expired_removal=True) # Delete and update stat\n",
    "            self._stats['misses'] += 1 # Treat as a miss\n",
    "            return None\n",
    "        \n",
    "        # 3. Update LRU order (move to end)\n",
    "        entry = self._data.pop(key)\n",
    "        self._data[key] = entry\n",
    "        \n",
    "        # 4. Update hit statistics\n",
    "        self._stats['hits'] += 1\n",
    "        \n",
    "        return entry['value']\n",
    "    \n",
    "    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Set value in cache.\n",
    "        \"\"\"\n",
    "        current_ttl = ttl if ttl is not None else self.default_ttl\n",
    "        \n",
    "        # 1. Calculate expiration time if TTL provided\n",
    "        expiry_time = None\n",
    "        if current_ttl is not None and current_ttl > 0:\n",
    "            expiry_time = time.time() + current_ttl\n",
    "        elif current_ttl is not None and current_ttl <= 0:\n",
    "            # If TTL is 0 or negative, treat it as immediate expiry or non-storage\n",
    "            return \n",
    "        elif current_ttl is None:\n",
    "            expiry_time = math.inf # Use a large value to signify permanent storage\n",
    "        \n",
    "        # Remove existing key to update LRU order and ensure new data is stored\n",
    "        if key in self._data:\n",
    "            self._data.pop(key)\n",
    "        \n",
    "        # 2. Check if cache is full and evict LRU items\n",
    "        if len(self._data) >= self.max_size:\n",
    "            # Evict the oldest (first) item\n",
    "            self._evict_lru(count=1)\n",
    "        \n",
    "        # 3. Store value with metadata\n",
    "        self._data[key] = {\n",
    "            'value': value,\n",
    "            'expiry': expiry_time\n",
    "        }\n",
    "        # 4. LRU order is automatically updated as it was popped/re-added or newly added\n",
    "        \n",
    "    def delete(self, key: str, is_expired_removal: bool = False) -> bool:\n",
    "        \"\"\"Delete key from cache.\"\"\"\n",
    "        if key in self._data:\n",
    "            del self._data[key]\n",
    "            \n",
    "            # Update expired removal stat only if called from _is_expired check\n",
    "            if is_expired_removal:\n",
    "                self._stats['expired_removals'] += 1\n",
    "                \n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    # --- Missing Management Methods ---\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all items from cache and reset statistics.\"\"\"\n",
    "        self._data.clear()\n",
    "        self._stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0,\n",
    "            'expired_removals': 0\n",
    "        }\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return current number of items in cache.\"\"\"\n",
    "        return len(self._data)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get cache statistics.\n",
    "        \n",
    "        Returns:\n",
    "            Dict with keys: hits, misses, evictions, expired_removals, current_size\n",
    "        \"\"\"\n",
    "        stats_copy = self._stats.copy()\n",
    "        stats_copy['current_size'] = self.size()\n",
    "        return stats_copy\n",
    "    \n",
    "    def cleanup_expired(self) -> int:\n",
    "        \"\"\"\n",
    "        Remove expired items from cache.\n",
    "        \n",
    "        Returns:\n",
    "            Number of items removed\n",
    "        \"\"\"\n",
    "        keys_to_remove = [key for key in self._data if self._is_expired(key)]\n",
    "        removed_count = 0\n",
    "        \n",
    "        for key in keys_to_remove:\n",
    "            # Delete calls the internal deletion logic, updating stats\n",
    "            self.delete(key, is_expired_removal=True) \n",
    "            removed_count += 1\n",
    "            \n",
    "        return removed_count\n",
    "    \n",
    "    def _evict_lru(self, count: int = 1) -> int:\n",
    "        \"\"\"\n",
    "        Evict least recently used items (from the beginning of OrderedDict).\n",
    "        \"\"\"\n",
    "        evicted_count = 0\n",
    "        for _ in range(count):\n",
    "            try:\n",
    "                # popitem(last=False) removes and returns the first (LRU) item\n",
    "                self._data.popitem(last=False) \n",
    "                self._stats['evictions'] += 1\n",
    "                evicted_count += 1\n",
    "            except KeyError:\n",
    "                # Cache is empty\n",
    "                break\n",
    "                \n",
    "        return evicted_count\n",
    "\n",
    "# Test your enhanced implementation\n",
    "if __name__ == \"__main__\":\n",
    "    # Test TTL functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=1)  # 1 second TTL\n",
    "    \n",
    "    print(\"=== Testing TTL ===\")\n",
    "    cache.set(\"temp_key\", \"temp_value\") # TTL is 1 sec\n",
    "    print(f\"Immediately after set (Hit): {cache.get('temp_key')}\")\n",
    "    time.sleep(1.1)\n",
    "    # The get() call should delete the expired item and return None\n",
    "    print(f\"After TTL expired (Miss/Removal): {cache.get('temp_key')}\") \n",
    "    \n",
    "    print(\"\\n=== Testing Size Limits & LRU ===\")\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1, ttl=None) # No expiration\n",
    "    cache.set(\"b\", 2, ttl=None)\n",
    "    cache.set(\"c\", 3, ttl=None)\n",
    "    print(f\"Cache size after adding 3 items: {cache.size()}\") # Expected: 3\n",
    "    \n",
    "    # Access 'a' to make it recently used (b, c, a order)\n",
    "    cache.get(\"a\") \n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4, ttl=None)\n",
    "    # Order should be c, a, d\n",
    "    print(f\"After adding 'd': a={cache.get('a')}, b={cache.get('b')}, c={cache.get('c')}, d={cache.get('d')}\")\n",
    "    assert cache.get('b') is None # 'b' should be evicted\n",
    "\n",
    "    print(\"\\n=== Testing Statistics ===\")\n",
    "    stats = cache.get_stats()\n",
    "    print(f\"Cache statistics: {stats}\")\n",
    "    \n",
    "    # Expected stats after tests:\n",
    "    # Hits: 2 (temp_key, a)\n",
    "    # Misses: 2 (temp_key after expiry, b after eviction)\n",
    "    # Evictions: 1 (b)\n",
    "    # Expired_Removals: 1 (temp_key)\n",
    "    \n",
    "    print(\"\\n=== Testing Cleanup ===\")\n",
    "    cache.set(\"expire_me_1\", \"value\", ttl=1)\n",
    "    cache.set(\"expire_me_2\", \"value\", ttl=1)\n",
    "    cache.set(\"permanent\", \"value\", ttl=None) # Will not expire\n",
    "    \n",
    "    time.sleep(1.1)\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    print(f\"Expired items removed: {removed_count}\") # Expected: 2\n",
    "    \n",
    "    final_stats = cache.get_stats()\n",
    "    print(f\"Final statistics: {final_stats}\")\n",
    "    assert final_stats['current_size'] == 1 # Only 'permanent' should remain\n",
    "    assert final_stats['expired_removals'] == 3 # 1 from TTL test + 2 from cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf0f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "# Question 9: Add Missing Features (Complex Class Enhancement)\n",
    "import time\n",
    "from typing import Any, Optional, Dict, Union\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"\n",
    "    Enhanced cache implementation with TTL, size limits, LRU eviction, and statistics.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_size: int = 100, default_ttl: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Initialize cache with size limit and default TTL.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.default_ttl = default_ttl\n",
    "        \n",
    "        # Use OrderedDict for combined data storage and LRU tracking. \n",
    "        # The order represents insertion/access time (LRU is at the beginning).\n",
    "        self._data: OrderedDict[str, Dict[str, Union[Any, float, int]]] = OrderedDict()\n",
    "        \n",
    "        # Statistics\n",
    "        self._stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0,\n",
    "            'expired_removals': 0\n",
    "        }\n",
    "        \n",
    "    def _is_expired(self, key: str) -> bool:\n",
    "        \"\"\"Check if a cache entry has expired.\"\"\"\n",
    "        entry = self._data.get(key)\n",
    "        if entry is None:\n",
    "            return False\n",
    "            \n",
    "        # Expiration time is stored as a timestamp (float)\n",
    "        expiry_time = entry.get('expiry')\n",
    "        \n",
    "        # If expiry is None or math.inf, it never expires\n",
    "        if expiry_time is None or expiry_time == math.inf:\n",
    "            return False\n",
    "            \n",
    "        return time.time() > expiry_time\n",
    "\n",
    "    def get(self, key: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Get value from cache.\n",
    "        \"\"\"\n",
    "        # 1. Check if key exists\n",
    "        if key not in self._data:\n",
    "            self._stats['misses'] += 1\n",
    "            return None\n",
    "        \n",
    "        # 2. Check if item has expired (TTL)\n",
    "        if self._is_expired(key):\n",
    "            # Treat as miss and delete it\n",
    "            self.delete(key, is_expired_removal=True) \n",
    "            self._stats['misses'] += 1 \n",
    "            return None\n",
    "        \n",
    "        # 3. Update LRU order (move to end)\n",
    "        entry = self._data.pop(key)\n",
    "        self._data[key] = entry\n",
    "        \n",
    "        # 4. Update hit statistics\n",
    "        self._stats['hits'] += 1\n",
    "        \n",
    "        return entry['value']\n",
    "    \n",
    "    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n",
    "        \"\"\"\n",
    "        Set value in cache, handling LRU eviction if max_size is reached.\n",
    "        \"\"\"\n",
    "        current_ttl = ttl if ttl is not None else self.default_ttl\n",
    "        \n",
    "        # 1. Calculate expiration time\n",
    "        expiry_time = None\n",
    "        if current_ttl is not None and current_ttl > 0:\n",
    "            expiry_time = time.time() + current_ttl\n",
    "        elif current_ttl is None:\n",
    "            expiry_time = math.inf # Permanent storage\n",
    "        else: # TTL is 0 or negative\n",
    "            return\n",
    "        \n",
    "        # Remove existing key to update LRU order/data\n",
    "        if key in self._data:\n",
    "            self._data.pop(key)\n",
    "        \n",
    "        # 2. Check if cache is full and evict LRU items\n",
    "        if len(self._data) >= self.max_size:\n",
    "            self._evict_lru(count=1) # Evicts the oldest item\n",
    "        \n",
    "        # 3. Store value with metadata (and updates LRU order implicitly)\n",
    "        self._data[key] = {\n",
    "            'value': value,\n",
    "            'expiry': expiry_time\n",
    "        }\n",
    "        \n",
    "    def delete(self, key: str, is_expired_removal: bool = False) -> bool:\n",
    "        \"\"\"Delete key from cache.\"\"\"\n",
    "        if key in self._data:\n",
    "            del self._data[key]\n",
    "            \n",
    "            if is_expired_removal:\n",
    "                self._stats['expired_removals'] += 1\n",
    "                \n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def clear(self) -> None:\n",
    "        \"\"\"Clear all items from cache and reset statistics.\"\"\"\n",
    "        self._data.clear()\n",
    "        self._stats = {\n",
    "            'hits': 0,\n",
    "            'misses': 0,\n",
    "            'evictions': 0,\n",
    "            'expired_removals': 0\n",
    "        }\n",
    "    \n",
    "    def size(self) -> int:\n",
    "        \"\"\"Return current number of items in cache.\"\"\"\n",
    "        return len(self._data)\n",
    "    \n",
    "    def get_stats(self) -> Dict[str, int]:\n",
    "        \"\"\"\n",
    "        Get cache statistics.\n",
    "        \"\"\"\n",
    "        stats_copy = self._stats.copy()\n",
    "        stats_copy['current_size'] = self.size()\n",
    "        return stats_copy\n",
    "    \n",
    "    def cleanup_expired(self) -> int:\n",
    "        \"\"\"\n",
    "        Remove expired items from cache.\n",
    "        \"\"\"\n",
    "        keys_to_remove = [key for key in self._data if self._is_expired(key)]\n",
    "        removed_count = 0\n",
    "        \n",
    "        for key in keys_to_remove:\n",
    "            self.delete(key, is_expired_removal=True) \n",
    "            removed_count += 1\n",
    "            \n",
    "        return removed_count\n",
    "    \n",
    "    def _evict_lru(self, count: int = 1) -> int:\n",
    "        \"\"\"\n",
    "        Evict least recently used items (from the beginning of OrderedDict).\n",
    "        \"\"\"\n",
    "        evicted_count = 0\n",
    "        for _ in range(count):\n",
    "            try:\n",
    "                # popitem(last=False) removes and returns the first (LRU) item\n",
    "                self._data.popitem(last=False) \n",
    "                self._stats['evictions'] += 1\n",
    "                evicted_count += 1\n",
    "            except KeyError:\n",
    "                break\n",
    "                \n",
    "        return evicted_count\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 9)\n",
    "import time\n",
    "\n",
    "def test_question_9():\n",
    "    print(\"Testing enhanced cache implementation...\")\n",
    "    \n",
    "    # Test 1: Basic functionality\n",
    "    cache = SimpleCache(max_size=3, default_ttl=60)\n",
    "    \n",
    "    cache.set(\"key1\", \"value1\")\n",
    "    cache.set(\"key2\", \"value2\")\n",
    "    \n",
    "    assert cache.get(\"key1\") == \"value1\", \"Basic get/set failed\"\n",
    "    assert cache.get(\"key2\") == \"value2\", \"Basic get/set failed\"\n",
    "    assert cache.size() == 2, f\"Expected size 2, got {cache.size()}\"\n",
    "    \n",
    "    # Test 2: TTL expiration\n",
    "    cache.clear()\n",
    "    cache.set(\"ttl_key\", \"ttl_value\", ttl=1)  # 1 second TTL\n",
    "    assert cache.get(\"ttl_key\") == \"ttl_value\", \"TTL key should be accessible immediately\"\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    assert cache.get(\"ttl_key\") is None, \"TTL key should be expired and return None\"\n",
    "    \n",
    "    # Test 3: Size limits and LRU eviction\n",
    "    cache.clear()\n",
    "    cache.set(\"a\", 1) # LRU: a\n",
    "    cache.set(\"b\", 2) # LRU: a, b\n",
    "    cache.set(\"c\", 3) # LRU: a, b, c. Cache is now full (max_size=3)\n",
    "    \n",
    "    # Access 'a' to make it recently used (b, c, a)\n",
    "    cache.get(\"a\")\n",
    "    \n",
    "    # Add 'd' which should evict 'b' (least recently used)\n",
    "    cache.set(\"d\", 4) # LRU: c, a, d\n",
    "    \n",
    "    assert cache.get(\"a\") == 1, \"Recently used 'a' should not be evicted\"\n",
    "    assert cache.get(\"b\") is None, \"Least recently used 'b' should be evicted\"\n",
    "    assert cache.get(\"c\") == 3, \"'c' should still be in cache\"\n",
    "    assert cache.get(\"d\") == 4, \"Newly added 'd' should be in cache\"\n",
    "    assert cache.size() == 3, \"Cache size should remain at max_size\"\n",
    "    \n",
    "    # Test 4: Statistics tracking\n",
    "    cache.clear()\n",
    "    cache.set(\"stat_key\", \"stat_value\")\n",
    "    cache.get(\"stat_key\")  # Hit 1\n",
    "    cache.get(\"nonexistent\")  # Miss 1\n",
    "    cache.set(\"evict_me\", 0) # LRU: stat_key, evict_me\n",
    "    cache.set(\"force_evict\", 0) # LRU: stat_key, evict_me, force_evict\n",
    "    cache.set(\"new\", 0) # Evicts stat_key, Evictions 1. LRU: evict_me, force_evict, new\n",
    "\n",
    "    stats = cache.get_stats()\n",
    "    required_stats = [\"hits\", \"misses\", \"evictions\", \"current_size\", \"expired_removals\"]\n",
    "    for stat in required_stats:\n",
    "        assert stat in stats, f\"Missing statistic: {stat}\"\n",
    "    \n",
    "    assert stats[\"hits\"] == 1, f\"Expected 1 hit, got {stats['hits']}\"\n",
    "    assert stats[\"misses\"] == 1, f\"Expected 1 miss, got {stats['misses']}\"\n",
    "    assert stats[\"evictions\"] == 1, f\"Expected 1 eviction, got {stats['evictions']}\"\n",
    "    assert stats[\"current_size\"] == 3, f\"Expected current size 3, got {stats['current_size']}\"\n",
    "    \n",
    "    # Test 5: Manual cleanup\n",
    "    cache.clear()\n",
    "    cache.set(\"expire1\", \"value1\", ttl=1)\n",
    "    cache.set(\"expire2\", \"value2\", ttl=1)\n",
    "    cache.set(\"keep\", \"value3\", ttl=None)  # No expiration\n",
    "    \n",
    "    time.sleep(1.1)  # Wait for expiration\n",
    "    removed_count = cache.cleanup_expired()\n",
    "    \n",
    "    assert removed_count == 2, f\"Should have removed 2 expired items, removed {removed_count}\"\n",
    "    assert cache.get(\"keep\") == \"value3\", \"Non-expiring item should remain\"\n",
    "    assert cache.size() == 1, \"Only one item should remain after cleanup\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    cache.clear()\n",
    "    assert cache.size() == 0, \"Cache should be empty after clear\"\n",
    "    assert cache.get(\"nonexistent\") is None, \"Getting non-existent key should return None\"\n",
    "    assert cache.delete(\"nonexistent\") == False, \"Deleting non-existent key should return False\"\n",
    "    \n",
    "    # Test delete functionality\n",
    "    cache.set(\"delete_me\", \"value\")\n",
    "    assert cache.delete(\"delete_me\") == True, \"Deleting existing key should return True\"\n",
    "    assert cache.get(\"delete_me\") is None, \"Deleted key should not be accessible\"\n",
    "    \n",
    "    print(\"✓ All Question 9 tests passed!\")\n",
    "\n",
    "# Execute the test\n",
    "test_question_9()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391df2c",
   "metadata": {},
   "source": [
    "## Question 10: Integration Challenge (Multiple Components)\n",
    "\n",
    "**Scenario:** You have three separate AI-generated modules that need to work together in a data processing pipeline, but they have interface mismatches and compatibility issues. Your job is to create the integration layer that makes them work together seamlessly.\n",
    "\n",
    "**Requirements:**\n",
    "- Create adapter/wrapper functions to handle data format conversions\n",
    "- Build a unified pipeline that chains all three components\n",
    "- Add comprehensive error handling for the integration\n",
    "- Handle edge cases and invalid data gracefully\n",
    "- Create helper functions for data transformation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "\n",
    "# Component 1: Data Processor (returns dict with specific structure)\n",
    "class DataProcessor:\n",
    "    \"\"\"AI Component 1 - processes raw data and returns structured dict\"\"\"\n",
    "    \n",
    "    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Process raw data and return structured dict.\"\"\"\n",
    "        if not isinstance(raw_data, list):\n",
    "            raise ValueError(\"Expected list input\")\n",
    "        \n",
    "        result = {\n",
    "            'total_items': len(raw_data),\n",
    "            'processed_items': [],\n",
    "            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}\n",
    "        }\n",
    "        \n",
    "        for item in raw_data:\n",
    "            # Check for dict and valid numeric value before processing\n",
    "            is_valid = isinstance(item, dict) and 'value' in item and isinstance(item['value'], (int, float))\n",
    "            \n",
    "            if is_valid:\n",
    "                result['processed_items'].append({\n",
    "                    'id': item.get('id', 'unknown'),\n",
    "                    'processed_value': item['value'] * 2,\n",
    "                    'original_value': item['value'],\n",
    "                    'status': 'processed'\n",
    "                })\n",
    "            else:\n",
    "                result['processed_items'].append({\n",
    "                    'id': 'error',\n",
    "                    'processed_value': 0,\n",
    "                    'original_value': item.get('value', None) if isinstance(item, dict) else None,\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Component 2: Analytics Engine (expects JSON string, returns tuple)\n",
    "class AnalyticsEngine:\n",
    "    \"\"\"AI Component 2 - performs analytics on data, expects JSON string input\"\"\"\n",
    "    \n",
    "    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:\n",
    "        \"\"\"Analyze data from JSON string, return (summary, metrics) tuple.\"\"\"\n",
    "        try:\n",
    "            data = json.loads(json_data_string)\n",
    "        except json.JSONDecodeError:\n",
    "            return None, \"Invalid JSON format\"\n",
    "        \n",
    "        if not isinstance(data, dict) or 'processed_items' not in data:\n",
    "            return None, \"Missing processed_items in data structure\"\n",
    "        \n",
    "        items = data['processed_items']\n",
    "        if not isinstance(items, list):\n",
    "            return None, \"processed_items must be a list\"\n",
    "        \n",
    "        # Extract numeric values for analysis\n",
    "        values = []\n",
    "        failed_count = 0\n",
    "        \n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and item.get('status') == 'processed':\n",
    "                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):\n",
    "                    values.append(item['processed_value'])\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        if not values:\n",
    "            total_items = len(items)\n",
    "            return None, f\"No valid numeric data found for analysis (Total items: {total_items})\"\n",
    "        \n",
    "        summary = f\"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)\"\n",
    "        metrics = {\n",
    "            'avg_value': sum(values) / len(values),\n",
    "            'max_value': max(values),\n",
    "            'min_value': min(values),\n",
    "            'total_value': sum(values),\n",
    "            'success_rate': len(values) / len(items) if items else 0.0\n",
    "        }\n",
    "        \n",
    "        return summary, metrics\n",
    "\n",
    "# Component 3: Report Generator (expects list of tuples, returns formatted string)\n",
    "class ReportGenerator:\n",
    "    \"\"\"AI Component 3 - generates reports from analytics results\"\"\"\n",
    "    \n",
    "    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:\n",
    "        \"\"\"Generate report from list of (summary, metrics) tuples.\"\"\"\n",
    "        if not isinstance(analytics_results_list, list):\n",
    "            return \"Error: Expected list input for report generation\"\n",
    "        \n",
    "        if not analytics_results_list:\n",
    "            return \"Error: No data provided for report generation\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"=\" * 50,\n",
    "            \"           ANALYSIS REPORT\",\n",
    "            \"=\" * 50\n",
    "        ]\n",
    "        \n",
    "        for i, result in enumerate(analytics_results_list):\n",
    "            report_lines.append(f\"\\n--- DATASET {i+1} ---\")\n",
    "\n",
    "            if not isinstance(result, tuple) or len(result) != 2:\n",
    "                report_lines.append(\"Analysis failed: Invalid data format - expected (summary, metrics) tuple\")\n",
    "                continue\n",
    "            \n",
    "            summary, metrics = result\n",
    "            \n",
    "            if summary is None:\n",
    "                report_lines.append(\"Analysis failed\")\n",
    "                report_lines.append(f\"  Error: {metrics}\")\n",
    "                continue\n",
    "            \n",
    "            report_lines.append(f\"Summary: {summary}\")\n",
    "            \n",
    "            if isinstance(metrics, dict):\n",
    "                report_lines.append(\"Metrics:\")\n",
    "                for key, value in metrics.items():\n",
    "                    if isinstance(value, float):\n",
    "                        report_lines.append(f\"    {key}: {value:.2f}\")\n",
    "                    else:\n",
    "                        report_lines.append(f\"    {key}: {value}\")\n",
    "            else:\n",
    "                report_lines.append(f\"Metrics: {metrics}\")\n",
    "        \n",
    "        report_lines.append(\"\\n\" + \"=\" * 50)\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# INTEGRATION FUNCTIONS (Solution)\n",
    "\n",
    "def dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert dictionary to JSON string for AnalyticsEngine.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use json.dumps to safely serialize the dictionary\n",
    "        return json.dumps(data_dict)\n",
    "    except TypeError:\n",
    "        # Handle cases where the dict contains unserializable types\n",
    "        raise ValueError(\"Data dictionary contains un-serializable types for JSON conversion.\")\n",
    "\n",
    "def validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate and clean raw input data.\n",
    "    Ensures the output is always a list of dictionaries.\n",
    "    \"\"\"\n",
    "    # If the input is not a list, wrap it in a list if it's a dict, otherwise return an empty list.\n",
    "    if not isinstance(raw_data, list):\n",
    "        if isinstance(raw_data, dict):\n",
    "            raw_data = [raw_data]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    # Simple cleaning: ensure every item in the list is a dictionary.\n",
    "    cleaned_list = [item if isinstance(item, dict) else {} for item in raw_data]\n",
    "    return cleaned_list\n",
    "\n",
    "def integrated_pipeline(raw_data_list: List[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Integrate all three components to process data end-to-end.\n",
    "    \"\"\"\n",
    "    # Step 1: Initialize components\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    analytics_results = []\n",
    "    \n",
    "    for i, raw_data in enumerate(raw_data_list):\n",
    "        try:\n",
    "            # Step 2: Validate and clean each raw dataset\n",
    "            cleaned_data = validate_and_clean_raw_data(raw_data)\n",
    "            \n",
    "            # Step 3: Process through DataProcessor\n",
    "            processed_dict = processor.process_data(cleaned_data)\n",
    "            \n",
    "            # Step 4: Convert results to JSON string (Adapter)\n",
    "            json_data = dict_to_json_adapter(processed_dict)\n",
    "            \n",
    "            # Step 5: Run analytics\n",
    "            analysis_result = analytics.analyze(json_data)\n",
    "            \n",
    "        except (ValueError, TypeError, Exception) as e:\n",
    "            # Gracefully handle component errors\n",
    "            analysis_result = (None, f\"Pipeline Error (Dataset {i+1}): {type(e).__name__}: {str(e)}\")\n",
    "            \n",
    "        # Step 6: Collect results\n",
    "        analytics_results.append(analysis_result)\n",
    "        \n",
    "    # Step 7: Generate final report\n",
    "    return reporter.generate_report(analytics_results)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test and Execution\n",
    "\n",
    "def create_sample_data() -> List[Any]:\n",
    "    \"\"\"Create sample test data for the pipeline.\"\"\"\n",
    "    return [\n",
    "        # Dataset 1: Normal data (List[Dict])\n",
    "        [\n",
    "            {'id': 'A1', 'value': 10},\n",
    "            {'id': 'A2', 'value': 20},\n",
    "            {'id': 'A3', 'value': 15}\n",
    "        ],\n",
    "        # Dataset 2: Smaller dataset (List[Dict])\n",
    "        [\n",
    "            {'id': 'B1', 'value': 5},\n",
    "            {'id': 'B2', 'value': 25}\n",
    "        ],\n",
    "        # Dataset 3: Mixed data with issues (List[Dict])\n",
    "        [\n",
    "            {'id': 'C1', 'value': 30},\n",
    "            {'id': 'C2'},  # Missing value\n",
    "            {'value': 40},  # Missing id\n",
    "            {'id': 'C4', 'value': 'invalid'},  # Invalid value type (will fail in DataProcessor)\n",
    "            'not_a_dict' # Invalid item type (will be cleaned to {})\n",
    "        ],\n",
    "        # Dataset 4: Error test - raw input is not a list/dict (should be cleaned to [])\n",
    "        \"this is a string, not data\", \n",
    "        # Dataset 5: Single dictionary input (should be wrapped by validate)\n",
    "        {'id': 'D1', 'value': 100} \n",
    "    ]\n",
    "\n",
    "# Test the integration\n",
    "if __name__ == \"__main__\":\n",
    "    import traceback\n",
    "    print(\"Testing component integration...\")\n",
    "    \n",
    "    print(\"\\n=== Testing Individual Components (Sanity Check) ===\")\n",
    "    \n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    # Test DataProcessor\n",
    "    test_data = [{'id': 'test', 'value': 10}, {'id': 'test2', 'value': 'bad'}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    print(f\"DataProcessor output: {processed}\")\n",
    "    \n",
    "    # Test AnalyticsEngine\n",
    "    json_data = json.dumps(processed)\n",
    "    analysis_result = analytics.analyze(json_data)\n",
    "    print(f\"AnalyticsEngine output: {analysis_result}\")\n",
    "    \n",
    "    # Test ReportGenerator\n",
    "    report = reporter.generate_report([analysis_result])\n",
    "    print(f\"ReportGenerator output:\\n{report}\")\n",
    "    \n",
    "    print(\"\\n=== Testing Integrated Pipeline (Full End-to-End) ===\")\n",
    "    \n",
    "    sample_datasets = create_sample_data()\n",
    "    \n",
    "    try:\n",
    "        final_report = integrated_pipeline(sample_datasets)\n",
    "        print(\"Integration successful!\")\n",
    "        print(final_report)\n",
    "    except Exception as e:\n",
    "        print(f\"Integration failed: {e}\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3064b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Dict, Any, Tuple, Optional, Union\n",
    "import traceback\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Component 1: Data Processor (returns dict with specific structure)\n",
    "class DataProcessor:\n",
    "    \"\"\"AI Component 1 - processes raw data and returns structured dict\"\"\"\n",
    "    \n",
    "    def process_data(self, raw_data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Process raw data and return structured dict.\"\"\"\n",
    "        if not isinstance(raw_data, list):\n",
    "            raise ValueError(\"Expected list input\")\n",
    "        \n",
    "        result = {\n",
    "            'total_items': len(raw_data),\n",
    "            'processed_items': [],\n",
    "            'metadata': {'processing_time': 0.1, 'timestamp': '2024-01-01T12:00:00Z'}\n",
    "        }\n",
    "        \n",
    "        for item in raw_data:\n",
    "            # Check for dict and valid numeric value before processing\n",
    "            is_valid = isinstance(item, dict) and 'value' in item and isinstance(item['value'], (int, float))\n",
    "            \n",
    "            if is_valid:\n",
    "                result['processed_items'].append({\n",
    "                    'id': item.get('id', 'unknown'),\n",
    "                    'processed_value': item['value'] * 2,\n",
    "                    'original_value': item['value'],\n",
    "                    'status': 'processed'\n",
    "                })\n",
    "            else:\n",
    "                result['processed_items'].append({\n",
    "                    'id': 'error',\n",
    "                    'processed_value': 0,\n",
    "                    'original_value': item.get('value', None) if isinstance(item, dict) else None,\n",
    "                    'status': 'failed'\n",
    "                })\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Component 2: Analytics Engine (expects JSON string, returns tuple)\n",
    "class AnalyticsEngine:\n",
    "    \"\"\"AI Component 2 - performs analytics on data, expects JSON string input\"\"\"\n",
    "    \n",
    "    def analyze(self, json_data_string: str) -> Tuple[Optional[str], Union[Dict[str, float], str]]:\n",
    "        \"\"\"Analyze data from JSON string, return (summary, metrics) tuple.\"\"\"\n",
    "        try:\n",
    "            data = json.loads(json_data_string)\n",
    "        except json.JSONDecodeError:\n",
    "            return None, \"Invalid JSON format\"\n",
    "        \n",
    "        if not isinstance(data, dict) or 'processed_items' not in data:\n",
    "            return None, \"Missing processed_items in data structure\"\n",
    "        \n",
    "        items = data['processed_items']\n",
    "        if not isinstance(items, list):\n",
    "            return None, \"processed_items must be a list\"\n",
    "        \n",
    "        # Extract numeric values for analysis\n",
    "        values = []\n",
    "        failed_count = 0\n",
    "        \n",
    "        for item in items:\n",
    "            if isinstance(item, dict) and item.get('status') == 'processed':\n",
    "                if 'processed_value' in item and isinstance(item['processed_value'], (int, float)):\n",
    "                    values.append(item['processed_value'])\n",
    "            else:\n",
    "                failed_count += 1\n",
    "        \n",
    "        if not values:\n",
    "            total_items = len(items)\n",
    "            return None, f\"No valid numeric data found for analysis (Total items: {total_items})\"\n",
    "        \n",
    "        summary = f\"Analyzed {len(items)} items ({len(values)} successful, {failed_count} failed)\"\n",
    "        metrics = {\n",
    "            'avg_value': sum(values) / len(values),\n",
    "            'max_value': max(values),\n",
    "            'min_value': min(values),\n",
    "            'total_value': sum(values),\n",
    "            'success_rate': len(values) / len(items) if items else 0.0\n",
    "        }\n",
    "        \n",
    "        return summary, metrics\n",
    "\n",
    "# Component 3: Report Generator (expects list of tuples, returns formatted string)\n",
    "class ReportGenerator:\n",
    "    \"\"\"AI Component 3 - generates reports from analytics results\"\"\"\n",
    "    \n",
    "    def generate_report(self, analytics_results_list: List[Tuple[Optional[str], Union[Dict, str]]]) -> str:\n",
    "        \"\"\"Generate report from list of (summary, metrics) tuples.\"\"\"\n",
    "        if not isinstance(analytics_results_list, list):\n",
    "            return \"Error: Expected list input for report generation\"\n",
    "        \n",
    "        if not analytics_results_list:\n",
    "            return \"Error: No data provided for report generation\"\n",
    "        \n",
    "        report_lines = [\n",
    "            \"=\" * 50,\n",
    "            \"           ANALYSIS REPORT\",\n",
    "            \"=\" * 50\n",
    "        ]\n",
    "        \n",
    "        for i, result in enumerate(analytics_results_list):\n",
    "            report_lines.append(f\"\\n--- DATASET {i+1} ---\")\n",
    "\n",
    "            if not isinstance(result, tuple) or len(result) != 2:\n",
    "                report_lines.append(\"Analysis failed: Invalid data format - expected (summary, metrics) tuple\")\n",
    "                continue\n",
    "            \n",
    "            summary, metrics = result\n",
    "            \n",
    "            if summary is None:\n",
    "                report_lines.append(\"Analysis failed\")\n",
    "                report_lines.append(f\"  Error: {metrics}\")\n",
    "                continue\n",
    "            \n",
    "            report_lines.append(f\"Summary: {summary}\")\n",
    "            \n",
    "            if isinstance(metrics, dict):\n",
    "                report_lines.append(\"Metrics:\")\n",
    "                for key, value in metrics.items():\n",
    "                    if isinstance(value, float):\n",
    "                        report_lines.append(f\"    {key}: {value:.2f}\")\n",
    "                    else:\n",
    "                        report_lines.append(f\"    {key}: {value}\")\n",
    "            else:\n",
    "                report_lines.append(f\"Metrics: {metrics}\")\n",
    "        \n",
    "        report_lines.append(\"\\n\" + \"=\" * 50)\n",
    "        return \"\\n\".join(report_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# INTEGRATION FUNCTIONS (Solution)\n",
    "\n",
    "def dict_to_json_adapter(data_dict: Dict[str, Any]) -> str:\n",
    "    \"\"\"\n",
    "    Convert dictionary to JSON string for AnalyticsEngine.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return json.dumps(data_dict)\n",
    "    except TypeError:\n",
    "        raise ValueError(\"Data dictionary contains un-serializable types for JSON conversion.\")\n",
    "\n",
    "def validate_and_clean_raw_data(raw_data: Any) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Validate and clean raw input data.\n",
    "    Ensures the output is always a list of dictionaries.\n",
    "    \"\"\"\n",
    "    if not isinstance(raw_data, list):\n",
    "        if isinstance(raw_data, dict):\n",
    "            raw_data = [raw_data]\n",
    "        else:\n",
    "            return []\n",
    "    \n",
    "    cleaned_list = [item if isinstance(item, dict) else {} for item in raw_data]\n",
    "    return cleaned_list\n",
    "\n",
    "def integrated_pipeline(raw_data_list: List[Any]) -> str:\n",
    "    \"\"\"\n",
    "    Integrate all three components to process data end-to-end.\n",
    "    \"\"\"\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    analytics_results = []\n",
    "    \n",
    "    for i, raw_data in enumerate(raw_data_list):\n",
    "        analysis_result: Tuple[Optional[str], Union[Dict[str, float], str]]\n",
    "        try:\n",
    "            # Step 1: Validate and clean\n",
    "            cleaned_data = validate_and_clean_raw_data(raw_data)\n",
    "            \n",
    "            # Step 2: Process\n",
    "            processed_dict = processor.process_data(cleaned_data)\n",
    "            \n",
    "            # Step 3: Convert to JSON (Adapter)\n",
    "            json_data = dict_to_json_adapter(processed_dict)\n",
    "            \n",
    "            # Step 4: Run analytics\n",
    "            analysis_result = analytics.analyze(json_data)\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Step 5: Gracefully handle errors\n",
    "            analysis_result = (None, f\"Pipeline Error (Dataset {i+1}): {type(e).__name__}: {str(e)}\")\n",
    "            \n",
    "        analytics_results.append(analysis_result)\n",
    "        \n",
    "    # Step 6: Generate final report\n",
    "    return reporter.generate_report(analytics_results)\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# Test Cell (Question 10)\n",
    "def test_question_10():\n",
    "    print(\"Testing integrated pipeline...\")\n",
    "    \n",
    "    # Test 1: Individual component functionality (Sanity check)\n",
    "    processor = DataProcessor()\n",
    "    analytics = AnalyticsEngine()\n",
    "    reporter = ReportGenerator()\n",
    "    \n",
    "    test_data = [{'id': 'test1', 'value': 10}, {'id': 'test2', 'value': 20}]\n",
    "    processed = processor.process_data(test_data)\n",
    "    \n",
    "    assert isinstance(processed, dict), \"DataProcessor should return dict\"\n",
    "    assert processed['total_items'] == 2, \"Should count items correctly\"\n",
    "    \n",
    "    json_data = json.dumps(processed)\n",
    "    summary, metrics = analytics.analyze(json_data)\n",
    "    \n",
    "    assert summary is not None, \"Analytics should return valid summary\"\n",
    "    assert isinstance(metrics, dict), \"Analytics should return metrics dict\"\n",
    "    assert metrics['avg_value'] == 30.0, \"Average processed value should be (20+40)/2 = 30\"\n",
    "    \n",
    "    report = reporter.generate_report([(summary, metrics)])\n",
    "    assert \"ANALYSIS REPORT\" in report, \"Report should contain header\"\n",
    "    \n",
    "    # Test 2: Data validation and cleaning\n",
    "    cleaned_data = validate_and_clean_raw_data([\n",
    "        {'id': 'valid', 'value': 10},\n",
    "        {'value': 20},  # Missing id\n",
    "        {'id': 'invalid'},  # Missing value (will be converted to failed item by DataProcessor)\n",
    "        'invalid_format'  # Wrong format (will be converted to {} by clean func)\n",
    "    ])\n",
    "    \n",
    "    assert isinstance(cleaned_data, list), \"Should return list\"\n",
    "    assert len(cleaned_data) == 4, \"Should keep all items but clean non-dicts\"\n",
    "    assert cleaned_data[3] == {}, \"Invalid item format should be cleaned to {}\"\n",
    "    \n",
    "    # Test 3: Integration adapters\n",
    "    test_dict = {'processed_items': [{'processed_value': 10}]}\n",
    "    json_str = dict_to_json_adapter(test_dict)\n",
    "    \n",
    "    assert isinstance(json_str, str), \"Should return JSON string\"\n",
    "    parsed = json.loads(json_str)\n",
    "    assert parsed == test_dict, \"Should preserve data structure\"\n",
    "    \n",
    "    # Test 4: Full pipeline integration\n",
    "    sample_datasets = [\n",
    "        [{'id': 'A1', 'value': 10}, {'id': 'A2', 'value': 20}], # Avg=30\n",
    "        [{'id': 'B1', 'value': 5}], # Avg=10\n",
    "        []  # Empty dataset (Analysis failed)\n",
    "    ]\n",
    "    \n",
    "    final_report = integrated_pipeline(sample_datasets)\n",
    "    \n",
    "    assert isinstance(final_report, str), \"Pipeline should return string report\"\n",
    "    assert \"ANALYSIS REPORT\" in final_report, \"Should contain report header\"\n",
    "    assert \"DATASET 1\" in final_report, \"Should have first section\"\n",
    "    assert \"DATASET 3\" in final_report, \"Should have third section\"\n",
    "    assert \"Avg_value: 30.00\" in final_report, \"Dataset 1 average check\"\n",
    "    assert \"Analysis failed\" in final_report, \"Dataset 3 (empty) should fail analysis\"\n",
    "    \n",
    "    # Test 5: Error handling (using the malformed data from Test 2)\n",
    "    malformed_report = integrated_pipeline([\n",
    "        [{'id': 'valid', 'value': 10}, 'invalid_format']\n",
    "    ])\n",
    "    \n",
    "    # DataProcessor will receive [{'id': 'valid', 'value': 10}, {}]\n",
    "    # DataProcessor output: 1 processed, 1 failed.\n",
    "    # Analytics should pass.\n",
    "    assert \"Analyzed 2 items (1 successful, 1 failed)\" in malformed_report, \"Should handle malformed data via cleaning\"\n",
    "    \n",
    "    # Test 6: Edge cases\n",
    "    edge_cases = [\n",
    "        [{'id': 'only_id'}],     # Missing value -> failed item. No valid values for analytics.\n",
    "    ]\n",
    "    \n",
    "    edge_report = integrated_pipeline(edge_cases)\n",
    "    assert \"No valid numeric data found for analysis\" in edge_report, \"Missing value should lead to analysis failure\"\n",
    "    \n",
    "    print(\"✓ All Question 10 tests passed!\")\n",
    "\n",
    "# Run the test\n",
    "test_question_10()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4fe67",
   "metadata": {},
   "source": [
    "## Final Submission Instructions\n",
    "\n",
    "### Before You Submit:\n",
    "\n",
    "**Code Quality Checklist:**\n",
    "- All test cells pass without errors\n",
    "- Code follows Python best practices and conventions  \n",
    "- Functions include appropriate documentation\n",
    "- Error handling is implemented where required\n",
    "- Edge cases are handled appropriately\n",
    "- Code is clean, readable, and maintainable\n",
    "\n",
    "**Save Your Work:**\n",
    "- **Save all code outputs** - Run all cells and keep the output visible\n",
    "- Save the notebook file (Ctrl+S / Cmd+S)\n",
    "- Verify all your implementations are in the correct code cells\n",
    "- Double-check that test cells show \"tests passed!\" messages\n",
    "\n",
    "### Submission Format:\n",
    "Submit your completed `firstname_lastname.ipynb` file with **all outputs preserved**. We want to see:\n",
    "- Your code implementations\n",
    "- Test results (passed/failed)\n",
    "- Any debugging output or print statements\n",
    "- Cell execution numbers\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
